{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafb990b-9bc3-456d-ba99-c0e4c0fa7ee8",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1f1fd-b51e-4782-b943-5e493780373b",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db48c65-3099-432d-a863-205bd7d1ae0a",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73fb26f-0bd9-46f0-8749-48a0202d212a",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale and transform features in a dataset to a specific range, usually between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1441cea-32b6-48b3-b6b0-919a30ecee20",
   "metadata": {},
   "source": [
    "Fromula:\n",
    "   \n",
    "X(scaled)=(x-x(min))/(x(max)-x(min))\n",
    "\n",
    "- X is the original value of a feature.\n",
    "- X(min) is the minimum value of that feature in the dataset.\n",
    "- X(max) is the maximum value of that feature in the dataset\n",
    "- X(scaled) is the scaled value of the feature after applying the transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0fd7-4372-4006-8f20-2411131fa179",
   "metadata": {},
   "source": [
    "Example :\n",
    "\n",
    "Suppose you have a dataset with a feature \"Age\" that ranges from 20 to 60 years, and another feature \"Income\" that ranges from $30,000  to  $100,000. These features have different scales, which might impact the performance of some machine learning algorithms. To address this, you can apply Min-Max scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816342ae-2938-42ab-81c3-d6f49b51f794",
   "metadata": {},
   "source": [
    "Original Data:\n",
    "\n",
    "Age: [20, 30, 40, 50, 60]\n",
    "\n",
    "Income: [$30,000, $40,000, $50,000, $80,000, $100,000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7871a6-6527-4f40-b4c6-82bf8ffc4b4a",
   "metadata": {},
   "source": [
    "Age : X(min)=20,X(max)=60\n",
    "\n",
    "Income : X(min)=30000 , X(max)=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fd375-3dad-4c91-a722-ea75c127dc31",
   "metadata": {},
   "source": [
    "Age: X(scaled)=0.25\n",
    "\n",
    "Income: X(scaled)=0.625"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75768f-a82f-489e-b31a-3b725926f8a2",
   "metadata": {},
   "source": [
    "Scaled Age: [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "Scaled Income: [0.00, 0.111, 0.222, 0.666, 1.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a62d58-d70f-4c35-a2c0-68532bbdb95c",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8900754-c2a5-4a93-b045-5be5604fd715",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as \"Vector Normalization,\" is a feature scaling method that involves transforming the values of each feature in a dataset to have a unit magnitude. In other words, it scales each feature's values such that they lie on the surface of a unit hypersphere. This technique is commonly used in scenarios where the direction of the data matters more than the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca985a7-751c-41f4-ae80-adfaa21b664c",
   "metadata": {},
   "source": [
    "X(scaled)=x/mod(x)\n",
    "\n",
    "X: is the original vector of feature values.\n",
    "mod(X):  represents the Euclidean norm (magnitude) of the vector X\n",
    "X(sclaed):is the scaled vector of feature values after applying the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55ae88-660e-4106-9d11-8437ba98e57b",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "Suppose you have a dataset with two features: \"Height\" (in inches) and \"Weight\" (in pounds). These features have different units and magnitudes. You want to apply the Unit Vector technique to scale them:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "Height: [65, 72, 58, 68, 70] (in inches)\n",
    "\n",
    "Weight: [150, 180, 120, 160, 170] (in pounds)\n",
    "\n",
    "Calculate Euclidean Norm for Each Data Point:\n",
    "\n",
    "For the first data point (65, 150)\n",
    "mod(X)=root((65)**2 + (150)**2)\n",
    "\n",
    "mod(X)=165.79\n",
    "\n",
    "Apply Unit Vector Scaling:\n",
    "\n",
    "For the first data point (65, 150): \n",
    "X(scaled)=(65,150)/165.79\n",
    "\n",
    "X(scaled)=(0.302,0.905)\n",
    "\n",
    "Scaled Height: [(0.392, 0.905), ...]\n",
    "\n",
    "Scaled Weight: [(0.639, 0.769), ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e90a3-d620-43f4-8e77-caa943bccab9",
   "metadata": {},
   "source": [
    "##### Differences between Min-Max Scaling and Unit Vector Scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7ba7f-6151-44e5-9c06-53adf782b0b6",
   "metadata": {},
   "source": [
    "##### Range:\n",
    "\n",
    "- Min-Max scaling transforms features to a specific range (e.g., 0 to 1).\n",
    "- Unit Vector scaling maintains the direction of the data while scaling it to have a unit magnitude.\n",
    "##### Magnitude Preservation:\n",
    "\n",
    "- Min-Max scaling adjusts values proportionally to maintain their relative distances.\n",
    "- Unit Vector scaling focuses on preserving the direction of data, which can be particularly useful in scenarios like text classification or when using similarity-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc7046-6d09-4d0c-a2d0-33bd8ae0afbf",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97d8b6-6711-40f0-8366-aee5c866a989",
   "metadata": {},
   "source": [
    "- PCA, which stands for Principal Component Analysis, is a widely used technique in the field of machine learning and statistics for dimensionality reduction and data visualization.\n",
    "- It aims to transform a high-dimensional dataset into a lower-dimensional representation while preserving the maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce8022-ffa3-478a-af45-ddbc029954fa",
   "metadata": {},
   "source": [
    "##### The steps involved in PCA are as follows:\n",
    "\n",
    "##### Standardize the Data: \n",
    "Ensure that the features have zero mean and unit variance, which is important for PCA's performance.\n",
    "\n",
    "##### Calculate the Covariance Matrix: \n",
    "Compute the covariance matrix of the standardized data to understand the relationships between different features.\n",
    "\n",
    "##### Compute Eigenvectors and Eigenvalues: \n",
    "The eigenvectors and eigenvalues of the covariance matrix represent the principal components and their corresponding variance. Eigenvectors give the directions of maximum variance, while eigenvalues indicate the amount of variance along each eigenvector.\n",
    "\n",
    "##### Select Principal Components: \n",
    "Sort the eigenvectors by their corresponding eigenvalues in descending order. Select the top \n",
    "k eigenvectors to retain the most significant variance (where k is the desired reduced dimensionality).\n",
    "\n",
    "#####  Projection: \n",
    "Create a new feature space using the selected eigenvectors as axes. Project the original data onto this new space to obtain the reduced-dimensional representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b6e82-143b-4748-b4f5-dee658c16fae",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "Suppose you have a dataset with two features: \"Height\" (in centimeters) and \"Weight\" (in kilograms) of individuals. You want to reduce this two-dimensional data to one dimension using PCA:\n",
    "\n",
    "1. Original Data:\n",
    "\n",
    "Height: [160, 175, 155, 180, 170] (in cm)\n",
    "\n",
    "Weight: [60, 70, 50, 75, 65] (in kg)\n",
    "\n",
    "2. Standardize the Data:\n",
    "Calculate the mean and standard deviation for each feature and standardize the data.\n",
    "\n",
    "3. Calculate Covariance Matrix: \n",
    "Compute the covariance matrix of the standardized data.\n",
    "\n",
    "4. Compute Eigenvectors and Eigenvalues:\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. Select Principal Component:\n",
    "If you want to reduce to one dimension (k=1), select the eigenvector corresponding to the highest eigenvalue.\n",
    "\n",
    "6. Projection: \n",
    "Project the original data onto the selected eigenvector.\n",
    "\n",
    "The reduced-dimensional data might look like this:\n",
    "\n",
    "Reduced Data: [0.82, 1.08, -1.05, 1.47, 0.59]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf61844-6ef6-4310-ae67-4ec64bd88097",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4f40f-3471-417e-9eca-4e7cfad7a3dd",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the context of dimensionality reduction and data representation. PCA can be used as a technique for feature extraction, where it transforms the original features into a new set of features, the principal components, which capture the most significant information and variance in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d97a5-e018-400c-880d-ff0da6d49638",
   "metadata": {},
   "source": [
    "#### PCA as Dimensionality Reduction:\n",
    "\n",
    "In its typical application, PCA is used to reduce the dimensionality of a dataset by identifying a lower-dimensional subspace that retains the most important information in the data. It achieves this by projecting the data onto a set of orthogonal axes (principal components) that capture the maximum variance.\n",
    "\n",
    "#### Feature Extraction using PCA:\n",
    "\n",
    "When applying PCA for feature extraction, the original features of the dataset are transformed into a new set of features represented by the principal components. These principal components are linear combinations of the original features and are ordered by their corresponding eigenvalues, indicating their importance in capturing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576d56d-273c-4d82-88f3-eeeeed6e5b50",
   "metadata": {},
   "source": [
    "#### Example: \n",
    "Original Data:\n",
    "\n",
    "Temperature: [25, 28, 22, 30, 27] (in Celsius)\n",
    "Humidity: [60, 65, 70, 55, 58] (in %)\n",
    "Pressure: [1012, 1008, 1015, 1005, 1010] (in hPa)\n",
    "\n",
    "- Standardize the Data: Calculate mean and standard deviation for each feature and standardize the data.\n",
    "\n",
    "- Compute Covariance Matrix and Eigenvectors: Compute the covariance matrix and its associated eigenvectors and eigenvalues.\n",
    "- Select Principal Components: If you want to extract two features (k=2), select the top two eigenvectors.\n",
    "\n",
    "- Projection: Project the original data onto the selected eigenvectors.\n",
    "\n",
    "The extracted features might look like this:\n",
    "\n",
    "Extracted Feature 1: [0.45, 0.23, -0.51, 0.83, 0.00]\n",
    "Extracted Feature 2: [0.29, -0.46, 0.65, -0.23, -0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6130e20c-6956-4f39-b061-2dd6664ca6a0",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b80eee-4ea9-4cab-bc58-9fd43640fcd0",
   "metadata": {},
   "source": [
    "The goal of Min-Max scaling preprocessing is to transform these features into a common range (usually between 0 and 1) so that they are on a similar scale and can be effectively used by machine learning algorithms without causing issues due to varying scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b42343-5042-42ea-8499-5dc12bee26bd",
   "metadata": {},
   "source": [
    "1. Understand the Data:\n",
    "\n",
    "First, analyze the range and distribution of each feature (price, rating, delivery time) in the dataset to determine if scaling is necessary. If the features have different scales and ranges, scaling might be beneficial.\n",
    "\n",
    "2. Apply Min-Max Scaling:\n",
    "\n",
    "X(scaled)=(x-x(min))/(x(max)-x(min))\n",
    "\n",
    "3. Update the Dataset:\n",
    "\n",
    "Replace the original values of each feature with their corresponding scaled values in the dataset.\n",
    "\n",
    "4. Use the Scaled Data:\n",
    "\n",
    "Once the data has been scaled, you can use it to train and test your recommendation system. Machine learning algorithms, including those used in recommendation systems, will now be able to work effectively with the scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94ccae-37a0-4c0d-8d8d-0afa5ffbacd2",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d2c9d-27fe-4bed-a6b1-baf165df9845",
   "metadata": {},
   "source": [
    "Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices can be beneficial, especially when dealing with a large number of features that might introduce noise or result in computational complexity. Here's how you can apply PCA to achieve dimensionality reduction for your stock price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d6c28-5b45-438d-81b7-8b3439668722",
   "metadata": {},
   "source": [
    "##### 1.Data Preparation:\n",
    "Gather your dataset, which includes features like company financial data and market trends. Ensure that the data is cleaned, normalized (if needed), and standardized so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "##### 2.Calculate Covariance Matrix:\n",
    "Compute the covariance matrix of the standardized dataset. The covariance matrix shows how different features relate to each other.\n",
    "\n",
    "#### 3.Eigenvalue Decomposition:\n",
    "Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues indicate the amount of variance along each eigenvector.\n",
    "\n",
    "#### 4.Select Principal Components:\n",
    "Sort the eigenvectors by their corresponding eigenvalues in decreasing order. This order reflects the importance of each principal component in capturing variance. Choose the top k eigenvectors that correspond to the most significant eigenvalues. These will be the principal components used for dimensionality reduction.\n",
    "\n",
    "#### 5.Project Data onto Principal Components:\n",
    "Transform the original data by projecting it onto the selected k principal components. This is achieved by calculating the dot product between the data and the eigenvector matrix. The result is a new dataset with reduced dimensionality.\n",
    "\n",
    "#### 6.Model Building:\n",
    "Train your stock price prediction model using the reduced-dimensional dataset. You can use various machine learning algorithms, such as regression, time series models, or neural networks, depending on the nature of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4f9d1-ef55-4bc2-9c59-47e95133a0a4",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ff36f77-091c-4a99-a37b-7007291cc6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the desired range\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "X_min = np.min(data)\n",
    "X_max = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling formula\n",
    "scaled_data = ((data - X_min) / (X_max - X_min)) * (b - a) + a\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "467e9aff-d4d4-4124-b1b1-8bf8fa496554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)  # Reshape to a column vector\n",
    "\n",
    "# Initialize MinMaxScaler with desired range\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\", data.flatten())\n",
    "print(\"Scaled Data:\", scaled_data.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fae71-97e8-4db4-8e53-4646098dc283",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3efc01-1977-4f82-8eac-94c798b11089",
   "metadata": {},
   "source": [
    "The decision of how many principal components to retain in a feature extraction using PCA depends on the specific goals of your analysis, the desired level of dimensionality reduction, and the amount of variance you're willing to retain. Here's a general approach to help you decide how many principal components to retain:\n",
    "\n",
    "1. **Calculate Cumulative Explained Variance:**\n",
    "   Compute the explained variance ratio for each principal component. The explained variance ratio represents the proportion of the total variance captured by each principal component. This can be calculated from the eigenvalues of the covariance matrix.\n",
    "\n",
    "2. **Plot Cumulative Explained Variance:**\n",
    "   Create a plot showing the cumulative explained variance as you add more principal components. This will help you visualize how much variance is retained as you increase the number of components.\n",
    "\n",
    "3. **Choose a Threshold:**\n",
    "   Decide on a threshold for the cumulative explained variance that you consider acceptable. For example, you might aim to retain 95% or 99% of the total variance.\n",
    "\n",
    "4. **Select Principal Components:**\n",
    "   Choose the number of principal components that correspond to the chosen threshold. This number will determine how many principal components you'll retain for feature extraction.\n",
    "\n",
    "The idea is to strike a balance between dimensionality reduction and retaining as much useful information as possible. If a small number of principal components capture a high percentage of the total variance, you might be able to achieve meaningful feature extraction with fewer components.\n",
    "\n",
    "Keep in mind that the nature of your data and your specific goals will influence the decision. Here are some considerations that might guide your choice:\n",
    "\n",
    "- **Explained Variance:** If a few principal components capture a large percentage of the variance, you might consider retaining those components to maintain most of the dataset's information.\n",
    "\n",
    "- **Dimensionality Reduction:** If the goal is to reduce dimensionality for visualization, analysis, or model training, you might choose to retain fewer components to achieve the desired reduction.\n",
    "\n",
    "- **Model Performance:** If the goal is to improve the performance of a machine learning model, you can experiment with different numbers of components and evaluate how model performance changes.\n",
    "\n",
    "- **Interpretability:** Fewer components might lead to more interpretable features, while more components might lead to more abstract and less interpretable features.\n",
    "\n",
    "As an example, if you find that the first two principal components capture a significant portion of the total variance (e.g., around 90%), you might choose to retain those two components. However, the exact number of principal components to retain will depend on the trade-off you're willing to make between dimensionality reduction and information retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e82304-b286-4d23-83ff-9bf4a2e728af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88c6a6-73fa-46ab-bb65-1359a5283c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1219ffb0-3046-4bee-a08d-e55ff487ff8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204781c8-81e4-4606-8385-cd78a4ff90df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156ca53-e695-490e-b6e1-669724600cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42d82e-f1de-428d-9569-4f6c06e06965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb2dab-e04b-4551-8d5d-d80fa5e3e9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cd537-a9c9-47ec-a727-889a217639c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b42081-b1d0-49d5-9152-9fa20a42a3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b595b-45ff-4d00-a2b9-77a243e689d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fde2d-d36c-4b4e-90b9-0bc73fee70ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a8a0e-167f-4c1b-85eb-b77d0af26412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c9f51-f54a-4426-b2ec-6156795ca1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
